{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import collections\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import boardgame3\n",
    "from boardgame3 import BLACK, WHITE\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x, filters, kernel_sizes=3, strides=1, activations='relu',\n",
    "            regularizer=keras.regularizers.l2(1e-4)):\n",
    "    shortcut = x\n",
    "    for i, filte in enumerate(filters):\n",
    "        kernel_size = kernel_sizes if isinstance(kernel_sizes, int) \\\n",
    "                else kernel_sizes[i]\n",
    "        stride = strides if isinstance(strides, int) else strides[i]\n",
    "        activation = activations if isinstance(activations, str) \\\n",
    "                else activations[i]\n",
    "        z = keras.layers.Conv2D(filte, kernel_size, strides=stride,\n",
    "                padding='same', kernel_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer)(x)\n",
    "        y = keras.layers.BatchNormalization()(z)\n",
    "        if i == len(filters) - 1:\n",
    "            y = keras.layers.Add()([shortcut, y])\n",
    "        x = keras.layers.Activation(activation)(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroAgent:\n",
    "    def __init__(self, env, batches=1, batch_size=4096,\n",
    "            kwargs={}, load=None, sim_count=800,\n",
    "            c_init=1.25, c_base=19652., prior_exploration_fraction=0.25):\n",
    "        self.env = env\n",
    "        self.board = np.zeros_like(env.board)\n",
    "        self.batches = batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.net = self.build_network(**kwargs)\n",
    "        self.reset_mcts()\n",
    "        self.sim_count = sim_count # MCTS 次数\n",
    "        self.c_init = c_init # PUCT 系数\n",
    "        self.c_base = c_base # PUCT 系数\n",
    "        self.prior_exploration_fraction = prior_exploration_fraction\n",
    "    \n",
    "    \n",
    "    def build_network(self, conv_filters, residual_filters, policy_filters,\n",
    "            learning_rate=0.001, regularizer=keras.regularizers.l2(1e-4)):\n",
    "        # 公共部分\n",
    "        inputs = keras.Input(shape=self.board.shape)\n",
    "        x = keras.layers.Reshape(self.board.shape + (1,))(inputs)\n",
    "        for conv_filter in conv_filters:\n",
    "            z = keras.layers.Conv2D(conv_filter, 3, padding='same',\n",
    "                    kernel_regularizer=regularizer,\n",
    "                    bias_regularizer=regularizer)(x)\n",
    "            y = keras.layers.BatchNormalization()(z)\n",
    "            x = keras.layers.ReLU()(y)\n",
    "        for residual_filter in residual_filters:\n",
    "            x = residual(x, filters=residual_filter, regularizer=regularizer)\n",
    "        intermediates = x\n",
    "        \n",
    "        # 概率部分\n",
    "        for policy_filter in policy_filters:\n",
    "            z = keras.layers.Conv2D(policy_filter, 3, padding='same',\n",
    "                    kernel_regularizer=regularizer,\n",
    "                    bias_regularizer=regularizer)(x)\n",
    "            y = keras.layers.BatchNormalization()(z)\n",
    "            x = keras.layers.ReLU()(y)\n",
    "        logits = keras.layers.Conv2D(1, 3, padding='same',\n",
    "                kernel_regularizer=regularizer, bias_regularizer=regularizer)(x)\n",
    "        flattens = keras.layers.Flatten()(logits)\n",
    "        softmaxs = keras.layers.Softmax()(flattens)\n",
    "        probs = keras.layers.Reshape(self.board.shape)(softmaxs)\n",
    "        \n",
    "        # 价值部分\n",
    "        z = keras.layers.Conv2D(1, 3, padding='same',\n",
    "                kernel_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer)(intermediates)\n",
    "        y = keras.layers.BatchNormalization()(z)\n",
    "        x = keras.layers.ReLU()(y)\n",
    "        flattens = keras.layers.Flatten()(x)\n",
    "        vs = keras.layers.Dense(1, activation=keras.activations.tanh,\n",
    "                kernel_regularizer=regularizer,\n",
    "                bias_regularizer=regularizer)(flattens)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=[probs, vs])\n",
    "\n",
    "        def categorical_crossentropy_2d(y_true, y_pred):\n",
    "            labels = tf.reshape(y_true, [-1, self.board.size])\n",
    "            preds = tf.reshape(y_pred, [-1, self.board.size])\n",
    "            return keras.losses.categorical_crossentropy(labels, preds)\n",
    "\n",
    "        loss = [categorical_crossentropy_2d, keras.losses.MSE]\n",
    "        optimizer = keras.optimizers.Adam(learning_rate)\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def reset_mcts(self):\n",
    "        def zero_board_factory(): # 用于构造 default_dict\n",
    "            return np.zeros_like(self.board, dtype=float)\n",
    "        self.q = collections.defaultdict(zero_board_factory)\n",
    "            # q值估计: board -> board\n",
    "        self.count = collections.defaultdict(zero_board_factory)\n",
    "            # q值计数: board -> board\n",
    "        self.policy = {} # 策略: board -> board\n",
    "        self.valid = {} # 有效位置: board -> board\n",
    "        self.winner = {} # 赢家: board -> None or int\n",
    "    \n",
    "        \n",
    "    def decide(self, observation, greedy=False, return_prob=False):\n",
    "        # 计算策略\n",
    "        board, player = observation\n",
    "        canonical_board = player * board#当player=1,则不变，当player=-1，则翻转\n",
    "        s = boardgame3.strfboard(canonical_board)\n",
    "        while self.count[s].sum() < self.sim_count: # 多次 MCTS 搜索\n",
    "            self.search(canonical_board, prior_noise=True,action=None)\n",
    "        prob = self.count[s] / self.count[s].sum()\n",
    "\n",
    "        # 采样\n",
    "        location_index = np.random.choice(prob.size, p=prob.reshape(-1))\n",
    "        location = np.unravel_index(location_index, prob.shape)\n",
    "        if return_prob:\n",
    "            return location, prob\n",
    "        return location\n",
    "    \n",
    "    \n",
    "    def learn(self, dfs):\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "        for batch in range(self.batches):\n",
    "            indices = np.random.choice(len(df), size=self.batch_size)\n",
    "            players, boards, probs, winners = (np.stack(\n",
    "                    df.loc[indices, field]) for field in df.columns)\n",
    "            canonical_boards = players[:, np.newaxis, np.newaxis] * boards\n",
    "            vs = (players * winners)[:, np.newaxis]\n",
    "            self.net.fit(canonical_boards, [probs, vs], verbose=0) # 训练\n",
    "        self.reset_mcts()\n",
    "    \n",
    "    \n",
    "    def search(self, board, prior_noise=False,action=None): # MCTS 搜索\n",
    "        s = boardgame3.strfboard(board)\n",
    "        \n",
    "        if s not in self.winner:\n",
    "            self.winner[s] = self.env.get_winner((board, BLACK),action=None) # 计算赢家\n",
    "        if self.winner[s] is not None: # 赢家确定的情况\n",
    "            return self.winner[s]\n",
    "        \n",
    "        if s not in self.policy: # 未计算过策略的叶子节点\n",
    "            pis, vs = self.net.predict(board[np.newaxis])\n",
    "            pi, v = pis[0], vs[0]\n",
    "            valid = self.env.get_valid((board, BLACK))\n",
    "            masked_pi = pi * valid\n",
    "            total_masked_pi = np.sum(masked_pi)\n",
    "            if total_masked_pi <= 0: # 所有的有效动作都没有概率，偶尔可能发生\n",
    "                masked_pi = valid # workaround\n",
    "                total_masked_pi = np.sum(masked_pi)\n",
    "            self.policy[s] = masked_pi / total_masked_pi\n",
    "            self.valid[s] = valid\n",
    "            return v\n",
    "        \n",
    "        # PUCT 上界计算\n",
    "        count_sum = self.count[s].sum()\n",
    "        coef = (self.c_init + np.log1p((1 + count_sum) / self.c_base)) * \\\n",
    "                math.sqrt(count_sum) / (1. + self.count[s])\n",
    "        if prior_noise: # 先验噪声\n",
    "            alpha = 1. / self.valid[s].sum()\n",
    "            noise = np.random.gamma(alpha, 1., board.shape)\n",
    "            noise *= self.valid[s]\n",
    "            noise /= noise.sum()\n",
    "            prior = (1. - self.prior_exploration_fraction) * \\\n",
    "                    self.policy[s] + \\\n",
    "                    self.prior_exploration_fraction * noise\n",
    "        else:\n",
    "            prior = self.policy[s]\n",
    "        ub = np.where(self.valid[s], self.q[s] + coef * prior, np.nan)\n",
    "        location_index = np.nanargmax(ub)\n",
    "        location = np.unravel_index(location_index, board.shape)\n",
    "        \n",
    "        (next_board, next_player), _, _, _ = self.env.next_step(\n",
    "                (board, BLACK), np.array(location))\n",
    "        next_canonical_board = next_player * next_board\n",
    "        next_v = self.search(next_canonical_board,prior_noise,location) # 递归搜索\n",
    "        v = next_player * next_v\n",
    "        \n",
    "        self.count[s][location] += 1\n",
    "        self.q[s][location] += (v - self.q[s][location]) / \\\n",
    "                self.count[s][location]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_board_factory(): # 用于构造 default_dict\n",
    "    return np.zeros_like(agent.board, dtype=float)\n",
    "agent.q = collections.defaultdict(zero_board_factory)\n",
    "    # q值估计: board -> board\n",
    "agent.count = collections.defaultdict(zero_board_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "board, player = observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "board=next_canonical_board#下一状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = boardgame3.strfboard(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s not in agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02789749, 0.02768102, 0.02759846, 0.02778037, 0.02784266,\n",
       "        0.02787567],\n",
       "       [0.02778945, 0.02751222, 0.02782781, 0.02791229, 0.02786498,\n",
       "        0.0278875 ],\n",
       "       [0.02732121, 0.02727047, 0.0276366 , 0.02788614, 0.02787465,\n",
       "        0.02790538],\n",
       "       [0.02769058, 0.02754864, 0.02761263, 0.02781018, 0.02788381,\n",
       "        0.02788375],\n",
       "       [0.02781133, 0.0277851 , 0.02775119, 0.02787532, 0.02788155,\n",
       "        0.02789157],\n",
       "       [0.02784739, 0.02786183, 0.02784718, 0.02787963, 0.02788218,\n",
       "        0.02789185]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pis, vs =agent.net.predict(board[np.newaxis])\n",
    "#model = keras.Model(inputs=inputs, outputs=[probs, vs])\n",
    "pi, v = pis[0], vs[0]#pi.shape=(10,10),v.shape=(1,)\n",
    "valid = agent.env.get_valid((board, BLACK))\n",
    "masked_pi = pi * valid\n",
    "total_masked_pi = np.sum(masked_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy[s] = masked_pi / total_masked_pi\n",
    "agent.valid[s] = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUCT 上界计算\n",
    "count_sum = agent.count[s].sum()\n",
    "coef = (agent.c_init + np.log1p((1 + count_sum) / agent.c_base)) * \\\n",
    "       math.sqrt(count_sum) / (1. + agent.count[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1. / agent.valid[s].sum()\n",
    "noise = np.random.gamma(alpha, 1., board.shape)\n",
    "noise *= agent.valid[s]\n",
    "noise /= noise.sum()\n",
    "prior = (1. - agent.prior_exploration_fraction) * \\\n",
    "        agent.policy[s] + \\\n",
    "        agent.prior_exploration_fraction * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub = np.where(agent.valid[s], agent.q[s] + coef * prior, np.nan)\n",
    "location_index = np.nanargmax(ub)#返回除了Nan之外的最大值的索引\n",
    "location = np.unravel_index(location_index, board.shape)\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "(next_board, next_player), _, _, _ = agent.env.next_step(\n",
    "    (board, BLACK), np.array(location))\n",
    "next_canonical_board = next_player * next_board#翻转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, -1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_canonical_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b1da856ddf12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "agent.count['ss'][(2,10)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(env, agent, return_trajectory=False, verbose=False):\n",
    "    if return_trajectory:\n",
    "        trajectory = []\n",
    "    observation = env.reset()\n",
    "    for step in itertools.count():#默认从1开始，步长为1的迭代器\n",
    "        board, player = observation\n",
    "        action, prob = agent.decide(observation, return_prob=True)\n",
    "        if verbose:\n",
    "            print(boardgame3.strfboard(board))\n",
    "            logging.info('第 {} 步：玩家 {}, 动作 {}'.format(step, player,\n",
    "                    action))\n",
    "        observation, winner, done, _ = env.step(action)\n",
    "        if return_trajectory:\n",
    "            trajectory.append((player, board, prob))\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(boardgame3.strfboard(observation[0]))\n",
    "                logging.info('赢家 {}'.format(winner))\n",
    "            break\n",
    "    if return_trajectory:\n",
    "        df_trajectory = pd.DataFrame(trajectory,\n",
    "                columns=['player', 'board', 'prob'])\n",
    "        df_trajectory['winner'] = winner\n",
    "        return df_trajectory\n",
    "    else:\n",
    "        return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('KInARow-v0', board_shape=6, target_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "小规模参数，用来初步求解比较小的问题\n",
    "\"\"\"\n",
    "train_iterations = 100\n",
    "train_episodes_per_iteration =100\n",
    "batches = 2\n",
    "batch_size = 64\n",
    "sim_count =200\n",
    "net_kwargs = {}\n",
    "net_kwargs['conv_filters'] = [256,]\n",
    "net_kwargs['residual_filters'] = [[256, 256],]\n",
    "net_kwargs['policy_filters'] = [256,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AlphaZeroAgent(env=env, kwargs=net_kwargs, sim_count=sim_count,\n",
    "        batches=batches, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "2021-01-22 14:45:45,327 [INFO] 第 0 步：玩家 1, 动作 (4, 4)\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++o+\n",
      "++++++\n",
      "2021-01-22 14:45:57,059 [INFO] 第 1 步：玩家 -1, 动作 (5, 0)\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++++\n",
      "++++o+\n",
      "x+++++\n",
      "2021-01-22 14:46:09,033 [INFO] 第 2 步：玩家 1, 动作 (4, 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-df5c560af965>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdfs_trajectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#经验\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_episodes_per_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         df_trajectory = self_play(env, agent,\n\u001b[0m\u001b[0;32m      6\u001b[0m                 return_trajectory=True, verbose=True)\n\u001b[0;32m      7\u001b[0m         logging.info('训练 {} 回合 {}: 收集到 {} 条经验'.format(\n",
      "\u001b[1;32m<ipython-input-5-8b5340cf8feb>\u001b[0m in \u001b[0;36mself_play\u001b[1;34m(env, agent, return_trajectory, verbose)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#默认从1开始，步长为1的迭代器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboardgame3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrfboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0a98be457ff1>\u001b[0m in \u001b[0;36mdecide\u001b[1;34m(self, observation, greedy, return_prob)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboardgame3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrfboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanonical_board\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim_count\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 多次 MCTS 搜索\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanonical_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0a98be457ff1>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, board, prior_noise, action)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 (board, BLACK), np.array(location))\n\u001b[0;32m    152\u001b[0m         \u001b[0mnext_canonical_board\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_player\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_board\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mnext_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_canonical_board\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprior_noise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 递归搜索\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_player\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0a98be457ff1>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, board, prior_noise, action)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 (board, BLACK), np.array(location))\n\u001b[0;32m    152\u001b[0m         \u001b[0mnext_canonical_board\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_player\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_board\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mnext_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_canonical_board\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprior_noise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 递归搜索\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_player\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0a98be457ff1>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, board, prior_noise, action)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 未计算过策略的叶子节点\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mpis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBLACK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1570\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m     \"\"\"\n\u001b[1;32m-> 1727\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m   def interleave(self,\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4120\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4122\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4123\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0;32m   4124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m-> 2938\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         if x is not None)\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m     \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    396\u001b[0m       \u001b[1;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m       \u001b[1;31m# and last_write_to_resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    528\u001b[0m       \u001b[1;31m# TODO(srbs): An alternate would be to just compare the old and new set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m       \u001b[1;31m# but that may not be as fast.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mupdated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_acd_resource_resolvers_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrites\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;31m# Conservatively remove any resources from `reads` that are also writes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_resource_resolver\u001b[1;34m(op, resource_reads, resource_writes)\u001b[0m\n\u001b[0;32m   4575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4576\u001b[0m   \u001b[0mupdated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4577\u001b[1;33m   if op.type in [\n\u001b[0m\u001b[0;32m   4578\u001b[0m       \u001b[1;34m\"DatasetToSingleElement\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetToTFRecord\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ReduceDataset\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4579\u001b[0m   ]:\n",
      "\u001b[1;32mH:\\Anacoda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2377\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2378\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2379\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(train_iterations):\n",
    "    # 自我对弈\n",
    "    dfs_trajectory = []#经验\n",
    "    for episode in range(train_episodes_per_iteration):\n",
    "        df_trajectory = self_play(env, agent,\n",
    "                return_trajectory=True, verbose=True)\n",
    "        logging.info('训练 {} 回合 {}: 收集到 {} 条经验'.format(\n",
    "                iteration, episode, len(df_trajectory)))\n",
    "        dfs_trajectory.append(df_trajectory)\n",
    "    # 利用经验进行学习\n",
    "    agent.learn(dfs_trajectory)\n",
    "    logging.info('训练 {}: 学习完成'.format(iteration))\n",
    "    self_play(env, agent,return_trajectory=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(dfs_trajectory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
